import { createArray, IterableStream } from '@paperdave/utils';
import { AuthOverride } from './api-key';
import { fetchOpenAI, FetchOptions } from './fetch';
import { ChatModel, PRICING_CHAT, PRICING_TEXT } from './models';
import { CompletionUsage, FinishReason, RawCompletionUsage, td } from './shared';
import { countChatPromptTokens, countTokens } from './tokenization';

export type GPTMessageRole = 'system' | 'user' | 'assistant';

export interface GPTMessage {
  /** The role of the author of this message. */
  role: GPTMessageRole;
  /** The contents of the message. */
  content: string;
  /** The name of the user in a multi-user chat. */
  name?: string;
}

export interface RawChatCompletion {
  id: string;
  object: 'chat.completion';
  /** Unix time in seconds. */
  created: number;
  model: ChatModel;
  usage: RawCompletionUsage;
  choices: RawChatCompletionChoice[];
}

export interface RawChatCompletionChoice {
  message: GPTMessage;
  finish_reason: FinishReason;
  index: number;
}

export interface RawChatCompletionChunk {
  id: string;
  object: 'chat.completion.chunk';
  /** Unix time in seconds. */
  created: number;
  model: ChatModel;
  choices: RawChatCompletionChunkChoice[];
}

export interface RawChatCompletionChunkChoice {
  delta: Partial<GPTMessage>;
  finish_reason: FinishReason | null;
  index: number;
}

export interface ChatCompletionOptions<Stream extends boolean = boolean, N extends number = number>
  extends FetchOptions {
  /**
   * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to
   * see all of your available models, or see our [Model overview](/docs/models/overview) for
   * descriptions of them.
   */
  model: ChatModel;
  /**
   * The messages to generate chat completions for, in the [chat
   * format](https://platform.openai.com/docs/guides/chat/introduction).
   */
  messages: GPTMessage[];
  /**
   * If set, partial message deltas will be sent, like in ChatGPT. Instead of being given a string,
   * you are given an async iterator of strings. Other metadata is available as a promise given on
   * the result object. See `ChatCompletionStream` for more details.
   */
  stream?: Stream;
  /**
   * The maximum number of [tokens](/tokenizer) to generate in the completion. The token count of
   * your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a
   * context length of 2048 tokens (except for the newest models, which support 4096).
   */
  maxTokens?: number | null;
  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
   * more random, while lower values like 0.2 will make it more focused and deterministic. We
   * generally recommend altering this or `top_p` but not both.
   */
  temperature?: number | null;
  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the model considers
   * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising
   * the top 10% probability mass are considered. We generally recommend altering this or
   * `temperature` but not both.
   */
  topP?: number | null;
  /**
   * How many completions to generate for each prompt. **Note:** Because this parameter generates
   * many completions, it can quickly consume your token quota. Use carefully and ensure that you
   * have reasonable settings for `max_tokens` and `stop`.
   */
  n?: N | null;
  /** Up to 4 sequences where the API will stop generating further tokens. */
  stop?: string | string[] | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
   * in the text so far, increasing the model's likelihood to talk about new topics. [See more
   * information about frequency and presence penalties.](/docs/api-reference/parameter-details)
   */
  presencePenalty?: number | null;
  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing
   * frequency in the text so far, decreasing the model's likelihood to repeat the same line
   * verbatim. [See more information about frequency and presence
   * penalties.](/docs/api-reference/parameter-details)
   */
  frequencyPenalty?: number | null;
  /**
   * Modify the likelihood of specified tokens appearing in the completion. Accepts a json object
   * that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value
   * from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both
   * GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits
   * generated by the model prior to sampling. The exact effect will vary per model, but values
   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100
   * should result in a ban or exclusive selection of the relevant token. As an example, you can
   * pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.
   */
  logitBias?: Record<string, number> | null;
  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect
   * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
   */
  user?: string;
  /** Number of retries before giving up. Defaults to 3. */
  retry?: number;
  auth?: AuthOverride;
}

export interface ChatCompletionMetadata {
  created: Date;
  model: ChatModel;
  usage: CompletionUsage;
}

export interface ChatCompletion extends ChatCompletionMetadata, ChatCompletionChoice {}

export interface ChatCompletionMulti extends ChatCompletionMetadata {
  choices: ChatCompletionChoice[];
}

export interface ChatCompletionStream {
  content: AsyncIterableIterator<string>;
  data: Promise<ChatCompletion>;
}

export interface ChatCompletionMultiStream {
  choices: AsyncIterableIterator<string>[];
  data: Promise<ChatCompletionMulti>;
}

export interface ChatCompletionChoice {
  content: string;
  finishReason: FinishReason;
}

export type ChatCompletionResultFromOptions<
  Stream extends boolean = false,
  N extends number = 1
> = false extends Stream
  ? 1 extends N
    ? ChatCompletion
    : ChatCompletionMulti
  : 1 extends N
  ? ChatCompletionStream
  : ChatCompletionMultiStream;

export async function generateChatCompletion<Stream extends boolean, N extends number>(
  options: ChatCompletionOptions<Stream, N>
): Promise<ChatCompletionResultFromOptions<Stream, N>> {
  const { retry, auth, ...gptOptions } = options;

  const response = await fetchOpenAI({
    endpoint: '/chat/completions',
    method: 'POST',
    body: {
      model: gptOptions.model,
      prompt: gptOptions.messages,
      max_tokens: gptOptions.maxTokens ?? (gptOptions as any).max_tokens,
      temperature: gptOptions.temperature,
      top_p: gptOptions.topP ?? (gptOptions as any).top_p,
      n: gptOptions.n,
      stop: gptOptions.stop,
      presence_penalty: gptOptions.presencePenalty ?? (gptOptions as any).presence_penalty,
      frequency_penalty: gptOptions.frequencyPenalty ?? (gptOptions as any).frequency_penalty,
      logit_bias: gptOptions.logitBias ?? (gptOptions as any).logit_bias,
      user: gptOptions.user,
    },
    auth,
    retry,
  });

  if (gptOptions.stream) {
    const reader = (response as any).body.getReader();
    if (!reader) {
      throw new Error('No Response');
    }
    const streams = createArray(gptOptions.n ?? 1, () => new IterableStream<string>());
    const completion: any = {};
    if (streams.length === 1) {
      completion.content = streams[0];
    } else {
      completion.choices = streams;
    }
    completion.data = (async () => {
      const choices = createArray(streams.length, () => ({ content: '' })) as any;
      const metadata: any = {
        created: null as any,
        model: null as any,
        usage: {
          promptTokens: 0,
          completionTokens: 0,
          totalTokens: 0,
          price: 0,
        },
      };
      try {
        let { done, value } = await reader.read();
        let buf = '';
        do {
          const lines = (td.decode(value) + buf).split('\n');
          buf = lines.pop()!;
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6);
              if (data !== '[DONE]') {
                const obj = JSON.parse(data) as RawChatCompletionChunk;
                if (obj.model && !metadata.model) {
                  metadata.model = obj.model;
                }
                if (obj.created && !metadata.created) {
                  metadata.created = new Date(obj.created * 1000);
                }
                for (const choice of obj.choices) {
                  if (choice.delta.content) {
                    streams[choice.index].push(choice.delta.content);
                    choices[choice.index].content += choice.delta.content;
                  }
                  if (choice.finish_reason) {
                    choices[choice.index].finishReason = choice.finish_reason;
                  }
                }
              }
            }
          }
          ({ done, value } = await reader.read());
        } while (!done);
      } finally {
        reader.releaseLock();
      }
      streams.forEach(stream => stream.end());
      metadata.usage.promptTokens = countChatPromptTokens(gptOptions.model, gptOptions.messages);
      metadata.usage.completionTokens = choices
        .map(x => countTokens(gptOptions.model, x.content))
        .reduce((a, b) => a + b, 0);
      metadata.usage.totalTokens = metadata.usage.promptTokens + metadata.usage.completionTokens;
      metadata.usage.price = PRICING_TEXT[metadata.model as ChatModel] * metadata.usage.totalTokens;
      if (choices.length === 1) {
        metadata.content = choices[0].content;
        metadata.finishReason = choices[0].finishReason;
      } else {
        metadata.choices = choices;
      }
      return metadata;
    })();
    return completion;
  }

  const json = (await response.json()) as RawChatCompletion;

  const completion: ChatCompletion | ChatCompletionMulti = {
    ...(json.choices.length === 1
      ? { content: json.choices[0].message.content, finishReason: json.choices[0].finish_reason }
      : {
          choices: json.choices.map(x => ({
            content: x.message.content,
            finishReason: x.finish_reason,
          })),
        }),
    created: new Date(json.created * 1000),
    model: json.model,
    usage: {
      promptTokens: json.usage.prompt_tokens,
      completionTokens: json.usage.completion_tokens,
      totalTokens: json.usage.total_tokens,
      price: json.usage.total_tokens * PRICING_CHAT[json.model],
    },
  };

  return completion as any;
}
